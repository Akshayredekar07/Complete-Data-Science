## **Vectors**

### 1. **Basic Vector Operations**
   - **Addition/Subtraction**: Combine vectors or find differences; adjust parameters and gradients.
   - **Scalar Multiplication**: Scale vectors to adjust weights or magnitude.
   - **Dot Product**: Measures similarity, used in SVMs and neural networks.
   - **Cross Product**: Creates a perpendicular vector in 3D.

### 2. **Vector Spaces**
   - **Vector Space**: A set of vectors that can be added/scaled; defines the operating space.
   - **Subspace**: A lower-dimensional subset.
   - **Basis/Dimension**: Basis is a set of independent vectors; dimension is the number of basis vectors.

### 3. **Norms and Distance Metrics**
   - **Norms**: Measures vector magnitude (e.g., L1, L2 norms) in optimization.
   - **Euclidean Distance**: Straight-line distance used in clustering.
   - **Cosine Similarity**: Measures angle between vectors, common in NLP.

### 4. **Linear Transformations**
   - **Matrix Representation**: Vectors transformed using matrices in models like neural networks.
   - **ML Transformations**: Data flows through transformations in models like linear regression.

### 5. **Feature Representation**
   - **Feature Vectors**: Numerical representations of data in multi-dimensional space.
   - **One-Hot Encoding**: Binary vector representation for categorical variables.

### 6. **Gradient and Optimization**
   - **Gradient Descent**: Optimization method using gradient vectors to minimize loss.
   - **Gradient Vector**: Direction of steepest ascent used in optimization.

### 7. **Neural Networks and SVMs**
   - **Weight Vectors**: Parameters in neural networks adjusted during training.
   - **Support Vectors (SVM)**: Define decision boundaries in classification.

### 8. **Embedding Vectors in NLP**
   - **Word Embeddings**: Dense vector representations of words (e.g., Word2Vec, GloVe).
   - **Sentence/Document Vectors**: Aggregated word vectors for text classification.

### 9. **High-Dimensional Spaces**
   - **Curse of Dimensionality**: Challenges with high-dimensional data.
   - **Dimensionality Reduction**: Techniques like PCA and t-SNE to reduce dimensions.

### 10. **Advanced Vector Concepts**
   - **Tensors**: Multi-dimensional vectors in deep learning.
   - **Eigenvectors/Eigenvalues**: Used in PCA for dimensionality reduction.

---

### 11. **Tensor Decomposition**
   - **SVD (Singular Value Decomposition)**: For dimensionality reduction and feature extraction.

### 12. **Optimization Techniques**
   - **SGD (Stochastic Gradient Descent)**: Common optimization method in deep learning.
   - **Adam, RMSProp**: Adaptive optimizers for faster convergence.

### 13. **Gradient Descent and Backpropagation**
   - **Gradient Descent**: Updates model parameters based on gradients.
   - **Backpropagation**: Trains neural networks by adjusting weights using gradients.

### 14. **Vector Embeddings**
   - **Word2Vec, GloVe**: Techniques for converting words into dense vectors.

### 15. **Support Vector Machines (SVM)**
   - **Support Vectors**: Key data points defining decision boundaries.
   - **Kernel Trick**: Transforms data for linear separation using vectors.

### 16. **Neural Networks**
   - **Weight Vectors**: Parameters in neural networks adjusted during training.
   - **Activation Vectors**: Outputs of neurons after applying activation functions.

### 17. **Dimensionality Reduction**
   - **PCA**: Projects data onto significant eigenvectors for reduced dimensionality.
   - **t-SNE**: Visualizes high-dimensional data in 2D/3D.

### 18. **Generative Models**
   - **GANs (Generative Adversarial Networks)**: Uses vector representations for data generation.
   - **VAEs (Variational Autoencoders)**: Learn latent vectors for generating data.

### 19. **Attention Mechanisms (NLP)**
   - **Self-Attention/Transformers**: Adjust input importance using vectors in NLP tasks.