### **Activation functions**

1. **Linear Activation**
2. **Sigmoid Activation**
3. **Tanh (Hyperbolic Tangent) Activation**
4. **ReLU (Rectified Linear Unit)**
5. **Leaky ReLU**
6. **Softmax Activation**
7. **ELU (Exponential Linear Unit)**
8. **Swish**
9. **GELU (Gaussian Error Linear Unit)**
10. **SELU (Scaled Exponential Linear Unit)**
11. **PReLU (Parametric ReLU)**
12. **Hard Sigmoid**
13. **Mish**
14. **ReLU6**
15. **Softplus**
